{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_dir)\n",
    "sys.path.append('../src')\n",
    "\n",
    "from config import MODEL_FP_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # 'gptj-6b',\n",
    "    # 'pythia-6.9b',\n",
    "    # 'llama-7b',\n",
    "    # 'llama2-7b',\n",
    "    # 'llama2i-7b',\n",
    "    # 'llama2-13b',\n",
    "    # 'llama2i-13b',\n",
    "    # 'llama2-70b',\n",
    "    # 'llama3_0-8b',\n",
    "    # 'llama3_0-70b',\n",
    "    # 'llama3-8b',\n",
    "    # 'llama3i-8b',\n",
    "    # 'llama3-70b',\n",
    "    # 'llama3i-70b',\n",
    "    # 'llama3.2-3b',\n",
    "    # 'mistral1-7b',\n",
    "    # 'mistral3-7b',\n",
    "    # 'mistral3i-7b',\n",
    "    # 'gemma2-2b',\n",
    "    # 'gemma2-9b',\n",
    "    # 'gemma2i-9b',\n",
    "    # 'gemma2-27b',\n",
    "    # 'qwen2-1.5b', \n",
    "    # 'qwen2-7b',\n",
    "    # 'qwen2i-7b',  \n",
    "    # 'qwen2-72b',\n",
    "    # 'qwen2.5-3b',\n",
    "    # 'qwen2.5i-3b',\n",
    "    # 'qwen2.5-7b',\n",
    "    # 'qwen2.5i-7b',\n",
    "    # 'qwen2.5-14b',\n",
    "    # 'olmo-7b-20BT',\n",
    "    # 'olmo-7b-50BT',\n",
    "    # 'olmo-7b-100BT',\n",
    "    # 'olmo-7b-2700BT',\n",
    "    # 'olmo-7b',\n",
    "    # 'olmoi-7b',\n",
    "    # 'olmo2-7b',\n",
    "    # 'olmo2i-7b',\n",
    "    # 'olmo2-13b',\n",
    "    # 'amber-7b-21BT',\n",
    "    # 'amber-7b-49BT',\n",
    "    # 'amber-7b-101BT',\n",
    "    # 'amber-7b',\n",
    "    # 'falcon3-7b',\n",
    "]\n",
    "tasks = [\n",
    "    # 'antonym',\n",
    "    # 'english-french',\n",
    "    # 'english-german',\n",
    "    # 'english-spanish',\n",
    "    # 'french-english',\n",
    "    # 'german-english',\n",
    "    # 'spanish-english',\n",
    "    # 'present-past',\n",
    "    # 'country-capital',\n",
    "    # 'tqa',\n",
    "]\n",
    "def get_slurm_cmd(model_name):\n",
    "    if float(model_name.split('-')[1][:-1]) >= 13:\n",
    "        n_gpu = '--gpus-per-node=1'\n",
    "        if float(model_name.split('-')[1][:-1]) > 27:\n",
    "            n_gpu = '--gpus-per-node=3'\n",
    "        cluster = '--cluster=your_cluster'\n",
    "        partition = '--partition=gpu_partition'\n",
    "        n_cpu = '--ntasks-per-node=2'\n",
    "        slurm_cmd = [cluster, partition, n_cpu, n_gpu]\n",
    "    else:\n",
    "        cluster = '--cluster=your_cluster'\n",
    "        partition = '--partition=gpu_partition'\n",
    "        n_gpu = '--gpus-per-node=1'\n",
    "        slurm_cmd = [cluster, partition, n_gpu]\n",
    "    return slurm_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = '0'\n",
    "max_samples = '1000'\n",
    "# task should be set to tqa in the previous cell \n",
    "for model in models:\n",
    "    slurm_cmd = get_slurm_cmd(model)\n",
    "    for task in tasks:\n",
    "        subprocess_arg = [\n",
    "            'sbatch', *slurm_cmd,\n",
    "            '../run_logit_lens.sh',\n",
    "            model, task, MODEL_FP_MAP[model], n_shots, max_samples\n",
    "        ]\n",
    "        subprocess.run(subprocess_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Indirect Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    slurm_cmd = get_slurm_cmd(model_name)\n",
    "    if float(model_name.split('-')[1][:-1]) >= 13:\n",
    "        runtime = '--time=04:00:00'\n",
    "    elif float(model_name.split('-')[1][:-1]) >= 6:\n",
    "        runtime = '--time=00:40:00'\n",
    "    else:\n",
    "        runtime = '--time=00:20:00'\n",
    "    slurm_cmd.append(runtime)\n",
    "    for task in tasks:\n",
    "        subprocess_arg = [\n",
    "            'sbatch', *slurm_cmd,\n",
    "            '../run_cie.sh',\n",
    "            model_name, task, MODEL_FP_MAP[model_name]\n",
    "        ]\n",
    "        subprocess.run(subprocess_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Function Vector Param Sweep\n",
    "#### Requires:\n",
    "1. Compute Indirect Effect\n",
    "- 1.5 hr param sweep smaller models\n",
    "- 2.5 hr param sweep larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    slurm_cmd = get_slurm_cmd(model_name)\n",
    "    for task in tasks:\n",
    "        subprocess_arg = [\n",
    "            'sbatch', *slurm_cmd,\n",
    "            '../run_fv_param_sweep.sh',\n",
    "            model_name, task, MODEL_FP_MAP[model_name]\n",
    "        ]\n",
    "        process = subprocess.run(subprocess_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Task Vectors\n",
    "- no new hyperparameters, follow instructions in README of icl_task_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run DoLa\n",
    "- please change the memory mapping in the ../DoLa/dola.py::load_model \"max_memory\" dictionary to value that work with your GPU setup (big models only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from typing import List\n",
    "def get_exit_layers(model_fp: str, bucket: List[int]):\n",
    "    config = AutoConfig.from_pretrained(model_fp)\n",
    "    lower = int(bucket[0]*config.num_hidden_layers)\n",
    "    upper = int(bucket[1]*config.num_hidden_layers) + 1\n",
    "    early_exit_layers = list(range(lower, upper, 2))\n",
    "    if early_exit_layers[-1] != config.num_hidden_layers:\n",
    "        early_exit_layers.append(config.num_hidden_layers)\n",
    "    return early_exit_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dola_tasks = ['factor'] # 'factor', 'tfqa'\n",
    "baselines = [False]\n",
    "ln_types = ['none'] # 'next_layer', 'last_layer' (experimental)\n",
    "# buckets = [\n",
    "#     [0, 0.25],\n",
    "#     [0.25, 0.5],\n",
    "#     [0.5, 0.75],\n",
    "#     [0.75, 1.0],\n",
    "# ]\n",
    "# buckets = [\n",
    "#     [0, 0.5],\n",
    "#     [0.25, 0.75],\n",
    "#     [0.5, 1.0],\n",
    "#     [0, 1.0],\n",
    "# ]\n",
    "buckets = [[0.0, 0.25]]\n",
    "alphas = ['0.0', '0.25', '0.5', '0.75', '0.9']\n",
    "# alphas = ['0.1']\n",
    "\n",
    "for model_name in models:\n",
    "    slurm_cmd = get_slurm_cmd(model_name)\n",
    "    n_gpu = slurm_cmd[-1][-1]\n",
    "    for baseline in baselines:\n",
    "        for bucket in buckets:\n",
    "            for alpha in alphas:\n",
    "                if baseline:\n",
    "                    early_exit_layers = []\n",
    "                else:\n",
    "                    early_exit_layers = get_exit_layers(MODEL_FP_MAP[model_name], bucket)\n",
    "                for dola_task in dola_tasks:\n",
    "                    for ln_type in ln_types:\n",
    "                        subprocess_arg = [\n",
    "                            'sbatch', *slurm_cmd, '--nodes=1',\n",
    "                            # '/fs/ess/PAS2836/pqd_localization/reliability-challenges-steering-lms/run_dola.sh',\n",
    "                            '../run_dola.sh',\n",
    "                            model_name, MODEL_FP_MAP[model_name], dola_task, n_gpu, alpha,\n",
    "                            ln_type, ','.join(map(str, early_exit_layers))\n",
    "                        ]\n",
    "                        print(subprocess_arg)\n",
    "                        process = subprocess.run(subprocess_arg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
