{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Times New Roman'\n",
    "\n",
    "from config import IMPLEMENTED_MODELS, MODEL_FP_MAP, STAGE_NAME_LONG, STAGE_NAME_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # 'gptj-6b',       \n",
    "    # 'pythia-6.9b',\n",
    "    # 'llama-7b',\n",
    "    # 'llama2-7b',     \n",
    "    # 'llama2i-7b',\n",
    "    # 'llama2-13b',     \n",
    "    # 'llama2i-13b',\n",
    "    # 'llama2-70b',\n",
    "    # 'llama3_0-8b',\n",
    "    # 'llama3-8b',\n",
    "    # 'llama3i-8b',\n",
    "    # 'llama3-70b',\n",
    "    # 'llama3i-70b',\n",
    "    # 'llama3.2-3b',  \n",
    "    # 'mistral1-7b',    \n",
    "    # 'mistral3-7b',    \n",
    "    # 'mistral3i-7b',\n",
    "    # 'gemma2-2b',      \n",
    "    'gemma2-9b',      \n",
    "    # 'gemma2i-9b',\n",
    "    # 'gemma2-27b',    \n",
    "    # 'qwen2-1.5b', \n",
    "    # 'qwen2-7b',      \n",
    "    # 'qwen2i-7b',  \n",
    "    # 'qwen2-72b',\n",
    "    # 'qwen2.5-3b',\n",
    "    # 'qwen2.5i-3b',\n",
    "    # 'qwen2.5-7b',\n",
    "    # 'qwen2.5i-7b',\n",
    "    # 'qwen2.5-14b',\n",
    "    # 'olmo-7b-20BT',\n",
    "    # 'olmo-7b-50BT',\n",
    "    # 'olmo-7b-100BT',\n",
    "    # 'olmo-7b-2700BT',\n",
    "    # 'olmo-7b',        \n",
    "    # 'olmos-7b',         \n",
    "    # 'olmoi-7b',\n",
    "    # 'olmo2-7b',\n",
    "    # 'olmo2i-7b',\n",
    "    # 'olmo2-13b',\n",
    "    # 'amber-7b-21BT',\n",
    "    # 'amber-7b-49BT',\n",
    "    # 'amber-7b-101BT',\n",
    "    # 'amber-7b',\n",
    "    # 'falcon3-7b',\n",
    "]\n",
    "\n",
    "data_names = [\n",
    "    # 'antonym',\n",
    "    # 'english-french',\n",
    "    # 'english-german',\n",
    "    # 'english-spanish',\n",
    "    # 'french-english',\n",
    "    # 'german-english',\n",
    "    # 'spanish-english',\n",
    "    # 'present-past',\n",
    "    # 'country-capital',\n",
    "    # 'colors',\n",
    "    'tqa',\n",
    "]\n",
    "def get_task_accuracy(model_name, data_name):\n",
    "    acc_fp = f'../function_vectors/logit_lens/results/token_probs/{model_name}/{data_name}/token_rank_by_layer.csv'\n",
    "    token_rank_by_layer = pd.read_csv(acc_fp)\n",
    "    last_layer = token_rank_by_layer['layerid'].max()\n",
    "    last_layer_token_ranks = token_rank_by_layer.query(\n",
    "        f\"layerid == {last_layer}\"\n",
    "    )\n",
    "    task_acc = np.mean(last_layer_token_ranks['correct_rank'] == 1)\n",
    "    return task_acc, last_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {\n",
    "    'Predicted': '#44AA99',\n",
    "    'Correct': '#117733',\n",
    "    'Incorrect': '#CC6677',\n",
    "    'Top': '#5C5C5D',\n",
    "}\n",
    "for model_name in models:\n",
    "    for data_name in data_names:\n",
    "        ll_fp = f'../function_vectors/logit_lens/results/token_probs/{model_name}/{data_name}/token_probabilities.csv'\n",
    "        if not os.path.exists(ll_fp):\n",
    "            continue\n",
    "        task_acc, n_layers = get_task_accuracy(model_name, data_name)\n",
    "        token_probs = pd.read_csv(ll_fp)\n",
    "        token_probs = token_probs.rename(columns={'pred_prob': 'Predicted', 'correct_prob': 'Correct', 'second_prob': 'Incorrect', 'top_prob': 'Top'})\n",
    "        to_plot = ['layerid', 'Top', 'Correct', 'Incorrect']\n",
    "        token_probs = pd.melt(\n",
    "            token_probs.loc[:, to_plot], id_vars='layerid', var_name='metric_name', value_name='metric'\n",
    "        )\n",
    "        plt.figure(figsize=(4, 2.5))\n",
    "        ax = sns.lineplot(data=token_probs, x='layerid', y='metric', hue='metric_name', style='metric_name', palette=palette, linewidth=2)\n",
    "        ax.set_title(f'{STAGE_NAME_LONG[model_name]}', fontsize=20)# | task_acc = {task_acc}')\n",
    "        ax.set_yticks(np.linspace(0, 1, 6))\n",
    "        try:\n",
    "            model_size = float(model_name.split('-')[-1][:-1])\n",
    "        except:\n",
    "            spaces = 8\n",
    "        if model_size > 13:\n",
    "            spaces = 16\n",
    "        else:\n",
    "            spaces = 8\n",
    "        ax.set_xticks(np.arange(0, n_layers+1, spaces))\n",
    "        ax.set_xlim([0, n_layers])\n",
    "        ax.tick_params('both', labelsize=12)\n",
    "        ax.set_xlabel('Layer', fontsize=12)\n",
    "        ax.set_ylabel('Token Probability', fontsize=15)\n",
    "        plt.legend(title='', framealpha=0, loc='lower left', bbox_to_anchor=(-0.02, 0), fontsize=13)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'../figures/logit_lens/{model_name}_tfqa.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(7,6))\n",
    "indices = [(0,0), (0,1), (1,0), (1,1)]\n",
    "data_name = data_names[0]\n",
    "for k, model_name in enumerate(['llama2-70b','llama3_0-70b','qwen2-7b', 'gemma2-9b']):\n",
    "    model_size = float(model_name.split('-')[1][:-1])\n",
    "    i, j = indices[k]\n",
    "    if i == 0 and j == 0:\n",
    "        legend = True\n",
    "    else:\n",
    "        legend = False\n",
    "\n",
    "    palette = {\n",
    "        # 'Pred. Token': '#000000',\n",
    "        'Correct': '#117733',\n",
    "        'Incorrect': '#CC6677',\n",
    "        'Top': '#5C5C5D',\n",
    "    }\n",
    "    ll_fp = f'../function_vectors/logit_lens/results/token_probs/{model_name}/{data_name}/token_probabilities.csv'\n",
    "    if not os.path.exists(ll_fp):\n",
    "        continue\n",
    "    task_acc, n_layers = get_task_accuracy(model_name, data_name)\n",
    "    token_probs = pd.read_csv(ll_fp)\n",
    "    token_probs = token_probs.rename(columns={'correct_prob': 'Correct', 'second_prob': 'Incorrect', 'top_prob': 'Top'})\n",
    "    to_plot = ['layerid', 'Top', 'Correct', 'Incorrect']\n",
    "    token_probs = pd.melt(\n",
    "        token_probs.loc[:, to_plot], id_vars='layerid', var_name='metric_name', value_name='metric'\n",
    "    )\n",
    "    \n",
    "    sns.lineplot(data=token_probs, x='layerid', y='metric', hue='metric_name', style='metric_name', ax=axs[i][j], linewidth=2.5, legend=legend, palette=palette)\n",
    "    \n",
    "    axs[i][j].set_yticks(np.arange(0, 1.1, 0.5))\n",
    "    if j == 0:\n",
    "        if i == 0:\n",
    "            incr = 16\n",
    "        if i == 1:\n",
    "            incr = 7\n",
    "    elif j == 1:\n",
    "        if i == 0:\n",
    "            incr = 16\n",
    "        if i == 1:\n",
    "            incr = 7\n",
    "    \n",
    "    axs[i][j].set_xticks(np.append(np.arange(0, n_layers, incr), n_layers))\n",
    "    # fontsize\n",
    "    axs[i][j].tick_params(axis='x', labelsize=25, rotation=0, length=12, width=1)\n",
    "    axs[i][j].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "    axs[i][j].set_title(STAGE_NAME_LONG[model_name], fontsize='25')\n",
    "    if model_size > 9:\n",
    "        axs[i][j].xaxis.set_minor_locator(MultipleLocator(2))\n",
    "    else:\n",
    "        axs[i][j].xaxis.set_minor_locator(MultipleLocator(1))\n",
    "    axs[i][j].tick_params(axis='x', which='minor', length=6, width=1)\n",
    "    axs[i][j].set_xlim(0, n_layers)\n",
    "\n",
    "    axs[i][j].set_xlabel('')\n",
    "    axs[i][j].set_ylabel('')\n",
    "    if j == 1:\n",
    "        axs[i][j].set_yticks([])\n",
    "        \n",
    "\n",
    "fig.supxlabel(r'Projected Layer ($\\ell$)', fontsize='25', y=0.045, x=0.55)\n",
    "fig.supylabel('Projected Token Probability', fontsize='25', x=0.03, y=0.56)\n",
    "axs[0][0].legend(loc='upper left', fontsize=17, framealpha=0, bbox_to_anchor=(-0.04, 1.08))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../figures/logit_lens/4_logit_lens_others.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Apathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import function_vectors.logit_lens.utils.ll_helpers as llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_idx = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "j_idx = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "fig, axs = plt.subplots(5, 2, figsize=(6,12))\n",
    "for i, model_name in enumerate(models):\n",
    "    for data_name in data_names:\n",
    "        if model_size > 13:\n",
    "            spaces = 32\n",
    "        else:\n",
    "            spaces = 8\n",
    "        ll_fp = f'../function_vectors/logit_lens/results/token_probs/{model_name}/{data_name}/layer_analyses.csv'\n",
    "        if not os.path.exists(ll_fp):\n",
    "            continue\n",
    "        layer_analyses, n_layers = get_task_accuracy(model_name, data_name)\n",
    "        layer_analyses = pd.read_csv(ll_fp)\n",
    "        layer_simple = layer_analyses[['layerid', 'hidden_name', 'apathy']]\n",
    "        layer_simple = layer_simple.groupby(['layerid', 'hidden_name']).agg('mean').reset_index()\n",
    "        layer_simple = layer_simple[layer_simple['hidden_name'].isin(['h_mha', 'h_mlp'])]\n",
    "        layer_simple['apathy'] = llh.minmax(layer_simple['apathy'])\n",
    "        metric_map = {'h_mha': 'MHA', 'h_mlp': 'MLP', 'Top': 'Top'}\n",
    "        layer_simple['hidden_name'] = layer_simple['hidden_name'].map(metric_map)\n",
    "        legend = True if i_idx[i] == 0 and j_idx[i] == 0 else False\n",
    "        sns.lineplot(data=layer_simple, x='layerid', y='apathy', hue='hidden_name', style='hidden_name', ax=axs[i_idx[i]][j_idx[i]], linewidth=2, legend=legend, dashes=[(4, 1), (1,1)])\n",
    "\n",
    "        ll_fp = f'../function_vectors/logit_lens/results/token_probs/{model_name}/{data_name}/token_probabilities.csv'\n",
    "        if not os.path.exists(ll_fp):\n",
    "            continue\n",
    "        task_acc, n_layers = get_task_accuracy(model_name, data_name)\n",
    "        token_probs = pd.read_csv(ll_fp)\n",
    "        token_probs = token_probs.rename(columns={'pred_prob': 'Predicted', 'correct_prob': 'Correct', 'second_prob': 'Incorrect', 'top_prob': 'Top'})\n",
    "        to_plot = ['layerid', 'Top']# , 'Correct', 'Incorrect']\n",
    "        token_probs = pd.melt(\n",
    "            token_probs.loc[:, to_plot], id_vars='layerid', var_name='metric_name', value_name='metric'\n",
    "        )\n",
    "        token_probs['metric'] = llh.minmax(token_probs['metric'])\n",
    "        palette = {\n",
    "            # 'Pred. Token': '#000000',\n",
    "            'Correct': '#117733',\n",
    "            'Incorrect': '#CC6677',\n",
    "            'Top': '#5C5C5D',\n",
    "        }\n",
    "        \n",
    "        \n",
    "        sns.lineplot(data=token_probs, x='layerid', y='metric', hue='metric_name', style='metric_name', palette=palette, linewidth=2, ax=axs[i_idx[i]][j_idx[i]], legend=legend)\n",
    "        \n",
    "        axs[i_idx[i]][j_idx[i]].set_title(f'{STAGE_NAME_LONG[model_name]}', fontsize=22)# | task_acc = {task_acc}')\n",
    "        axs[i_idx[i]][j_idx[i]].set_yticks(np.linspace(0, 1, 6))\n",
    "        axs[i_idx[i]][j_idx[i]].set_xticks(np.arange(0, n_layers+1, spaces))\n",
    "        axs[i_idx[i]][j_idx[i]].set_xlim([0, n_layers])\n",
    "        if j_idx[i] == 1:\n",
    "            axs[i_idx[i]][j_idx[i]].set_yticks([])\n",
    "        axs[i_idx[i]][j_idx[i]].set_ylabel('')\n",
    "        axs[i_idx[i]][j_idx[i]].set_xlabel('')\n",
    "        axs[i_idx[i]][j_idx[i]].tick_params('both', labelsize=15)\n",
    "        if legend:\n",
    "            axs[i_idx[i]][j_idx[i]].legend(title='', framealpha=0, loc='upper left', bbox_to_anchor=(0, 1), fontsize=13)\n",
    "        fig.supxlabel('Layer', fontsize=25)\n",
    "        fig.supylabel('Token Probability and Apathy', fontsize=25)\n",
    "        fig.suptitle('Apathy and Token Probabilities on TruthfulQA', fontsize=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'../figures/logit_lens/apathy.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline(model_name, data_name):\n",
    "    \"\"\"Read a JSON file and return its contents as a dictionary.\"\"\"\n",
    "    file_path = f\"../function_vectors/results/{model_name}/{data_name}/baseline_n_shots.json\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(8.5,7))\n",
    "data_names = ['tqa', 'present-past', 'present-past'] #country-capital\n",
    "fv_tv_tp = [ \n",
    "    [\n",
    "        ('olmo-7b', ''),\n",
    "        ('qwen2-72b', ''),\n",
    "        ('llama-7b', '')\n",
    "    ],\n",
    "    [\n",
    "        ('mistral3-7b', 'Mistral-7B-v0.3'),\n",
    "        # ('llama3-70b', 'Llama-3.1-70B'),\n",
    "        ('llama3i-70b', 'Llama-3.1-70B-Instruct'),\n",
    "        # ('mistral1-7b', 'Mistral-7B-v0.1'),\n",
    "        # ('gptj-6b', 'gpt-j-6b')\n",
    "        ('llama2-7b', 'Llama-2-7b-hf')\n",
    "    ],\n",
    "    [\n",
    "        # ('olmo-7b', 'OLMo-7B-0724-hf'),\n",
    "        ('pythia-6.9b', 'pythia-6.9b'),\n",
    "        ('qwen2.5-7b', 'Qwen2.5-7B'),\n",
    "        # ('qwen2i-7b', 'Qwen2-7B-Instruct'),\n",
    "        # ('olmoi-7b', 'OLMo-7B-0724-Instruct-hf'),\n",
    "        # ('qwen2.5-3b', 'Qwen2.5-3B'),\n",
    "        # ('llama3-8b', 'Meta-Llama-3.1-8B'),\n",
    "        #  ('qwen2-7b', 'Qwen2-7B'),\n",
    "        #  ('llama2-13b', 'Llama-2-13b-hf'),\n",
    "        #  ('llama-7b', 'llama-7b'),\n",
    "         ('gptj-6b', 'gpt-j-6b')\n",
    "    ]\n",
    "]\n",
    "heads_lambdas = [\n",
    "    ['016', '002'], # mistral3i\n",
    "    ['064', '001'], # llama3-70b\n",
    "    # ['016', '004'], # mistral1\n",
    "    # ['016', '004'], # gptj\n",
    "    ['002', '016'], # gptj\n",
    "]\n",
    "layers = [\n",
    "    [8, 16, 8],\n",
    "    [8, 16, 8],\n",
    "    [8, 7, 7],\n",
    "]\n",
    "palette = {\n",
    "    'Function Vector': '#000000',\n",
    "    'Task Vector': '#000000',\n",
    "    '0-shot': '#D55E00',\n",
    "    '5-shot': '#332288',\n",
    "    # For logit lens\n",
    "    'Correct': '#117733',\n",
    "    'Incorrect': '#CC6677',\n",
    "    'Top': '#5C5C5D',\n",
    "}\n",
    "for i, models in enumerate(fv_tv_tp):\n",
    "    data_name = data_names[i]\n",
    "    for j, (model_name, hf_name) in enumerate(models):\n",
    "        \n",
    "        # configs\n",
    "        model_fp = MODEL_FP_MAP[model_name]\n",
    "        model_config = json.load(open(os.path.join(model_fp, 'config.json')))\n",
    "        if model_name in ['gptj-6b']:\n",
    "            n_layers = model_config['n_layer']\n",
    "        else:\n",
    "            n_layers = model_config['num_hidden_layers']\n",
    "        # for plots\n",
    "        if i == 0:\n",
    "            ll_fp = f'../function_vectors/logit_lens/results/token_probs/{model_name}/{data_name}/token_probabilities.csv'\n",
    "            if not os.path.exists(ll_fp):\n",
    "                continue\n",
    "            task_acc, n_layers = get_task_accuracy(model_name, data_name)\n",
    "            token_probs = pd.read_csv(ll_fp)\n",
    "            token_probs = token_probs.rename(columns={'pred_prob': 'Predicted', 'correct_prob': 'Correct', 'second_prob': 'Incorrect', 'top_prob': 'Top'})\n",
    "            to_plot = ['layerid', 'Top', 'Correct', 'Incorrect']\n",
    "            token_probs = pd.melt(\n",
    "                token_probs.loc[:, to_plot], id_vars='layerid', var_name='metric_name', value_name='metric'\n",
    "            )\n",
    "            legend = True if j == 0 else False\n",
    "            sns.lineplot(data=token_probs, ax=axs[i][j], x='layerid', y='metric', hue='metric_name', style='metric_name', palette=palette, legend=legend, linewidth=3)\n",
    "            axs[i][j].set_xlabel('')\n",
    "            axs[i][j].set_ylabel('')\n",
    "        else:\n",
    "            # For function vectors, first row of plot\n",
    "            if i == 1:\n",
    "                baseline_n_shots = get_baseline(model_name, data_name)\n",
    "                # print(model_name, baseline_n_shots)\n",
    "                zero_shot = dict(zip([str(i) for i in range(n_layers)], [baseline_n_shots['0']] * n_layers))\n",
    "                five_shot = dict(zip([str(i) for i in range(n_layers)], [baseline_n_shots['5']] * n_layers))\n",
    "                saved_fname = f'{hf_name}_{data_name}_{heads_lambdas[j][0]}-top-heads_{heads_lambdas[j][1]}-lambda_50-test-samples'\n",
    "                dp = f\"../function_vectors/results/{model_name}/{data_name}/0_shot_w_FV\"\n",
    "                results_fp = os.path.join(dp, f\"{saved_fname}.json\")\n",
    "                if os.path.isfile(results_fp):\n",
    "                    zero_shot_fv = json.load(open(results_fp))\n",
    "                    to_plot = pd.DataFrame({\n",
    "                            'Function Vector': zero_shot_fv,\n",
    "                            '0-shot': zero_shot,\n",
    "                            '5-shot': five_shot,\n",
    "                        })           \n",
    "            if i == 2:\n",
    "                model_name = model_name.replace('-', '_')\n",
    "                if os.path.exists(f'../icl_task_vectors/for_fv/{model_name}.csv'):\n",
    "                    tv_df = pd.read_csv(f'../icl_task_vectors/for_fv/{model_name}.csv')\n",
    "                    baselines = pd.read_csv(f'../icl_task_vectors/for_fv/{model_name}_baselines.csv')\n",
    "                    model_name = model_name.replace('_', '-')\n",
    "                    zero_shot = baselines.loc[0, data_name]\n",
    "                    five_shot = baselines.loc[1, data_name]\n",
    "                    to_plot = pd.DataFrame({\n",
    "                        'Task Vector': tv_df[data_name],\n",
    "                        '0-shot': zero_shot,\n",
    "                        '5-shot': five_shot,\n",
    "                    })\n",
    "            legend = True if j == 0 else False\n",
    "            sns.lineplot(to_plot, ax=axs[i][j], legend=legend, linewidth=3, palette=palette)  \n",
    "        # axs[i][j].xaxis.set_minor_locator(MultipleLocator(1))\n",
    "        ylabels = ['{(DoLA)\\nToken Probability', '(Function Vectors)\\nICL Accuracy', '(Task Vectors)\\nICL Accuracy']\n",
    "        if j == 0:\n",
    "            axs[i][j].set_ylabel(ylabels[i], fontsize=18, rotation=90)\n",
    "            axs[i][j].yaxis.label.set_position((-1, 0.5))\n",
    "        # yticks\n",
    "        axs[i][j].yaxis.tick_right()\n",
    "        axs[i][j].set_yticks([0, 1])\n",
    "        if j in [0, 1]:\n",
    "            axs[i][j].set_yticklabels([])\n",
    "            axs[i][j].set_yticks([])\n",
    "        \n",
    "        axs[i][j].set_title(f'{STAGE_NAME_LONG[model_name]}', fontsize=20, rotation=0) #{STAGE_NAME_DATA[data_name]}\n",
    "        # xticks\n",
    "        axs[i][j].set_xticks(np.append(np.arange(0, n_layers, layers[i][j]), n_layers-1))\n",
    "        axs[i][j].set_xlim(0, n_layers-1)\n",
    "        # axs[i][j].tick_params(axis='x', which='minor', length=8, width=1)\n",
    "            \n",
    "            \n",
    "        axs[i][j].tick_params(axis='x', labelsize=15, length=6, width=1)\n",
    "        axs[i][j].tick_params(axis='y', labelsize=15)\n",
    "        if j == 0:\n",
    "            leg = axs[i][j].legend(\n",
    "                loc='upper left', fontsize=15.5, framealpha=0, bbox_to_anchor=(-.05, 0.95)\n",
    "            )\n",
    "            if i == 0:\n",
    "                leg = axs[i][j].legend(\n",
    "                    loc='upper left', fontsize=16, framealpha=0, bbox_to_anchor=(-.05, 1.0)\n",
    "                )\n",
    "    \n",
    "plt.suptitle(' ', fontsize=25)\n",
    "fig.text(0.365, .935, r'Unreliable Behavior', ha='center', fontsize=30)\n",
    "fig.text(0.833, .935, r'Prior Work', ha='center', fontsize=30)\n",
    "fig.supxlabel(r'Model layer ($\\ell$)', fontsize=20, y=0.03)\n",
    "\n",
    "plt.tight_layout(w_pad=0.0001)\n",
    "plt.savefig('../figures/figure1.pdf', dpi=300, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
